#!/usr/bin/env python3
"""
Run Nextflow with a wrapping weblog server.
"""
import argparse
import contextlib
import datetime
import json
import logging
import signal
import socket
import subprocess
import sys
import threading
import time
from email.mime.text import MIMEText
from http.server import BaseHTTPRequestHandler, HTTPServer
from pathlib import Path
from typing import Optional


@contextlib.contextmanager
def ignore_signals(*signals: int):
    """
    Ignore the given signals (once each) while inside this context.
    """
    old_handlers = {}
    logger = logging.getLogger("ignore_signals")

    def signal_handler(sig: int, _frame):
        logger.warning("Caught and ignoring %s!", signal.Signals(sig).name)
        # Restore the original handler
        logger.debug("Unmasking %s", signal.Signals(sig).name)
        signal.signal(sig, old_handlers.pop(sig))

    try:
        # Replace and hold onto the existing signal handlers
        for sig in signals:
            # The existing handler is _probably_ signal.SIG_DFL, but capture
            # the current value just in case it is not
            logger.debug("Masking %s", signal.Signals(sig).name)
            old_handlers[sig] = signal.signal(sig, signal_handler)
        yield
    finally:
        # Restore any handlers we masked
        for sig in list(old_handlers):
            logger.debug("Unmasking %s", signal.Signals(sig).name)
            signal.signal(sig, old_handlers.pop(sig))


def parse_time(timedict: Optional[dict]) -> Optional[datetime.datetime]:
    "Parse a Nextflow dictionary for a timestamp."
    # Ugh, python3.6 (the default on the cluster, despite reaching EOL in 2021)
    # doesn't have robust timezone handling. Parse out the time from these
    # dicts instead.
    if timedict is None:
        return None

    return datetime.datetime(
        year=timedict["year"],
        month=timedict["monthValue"],
        day=timedict["dayOfMonth"],
        hour=timedict["hour"],
        minute=timedict["minute"],
        second=timedict["second"],
        # microsecond=timedict["nano"] // 1000,
        tzinfo=datetime.timezone(
            offset=datetime.timedelta(
                seconds=timedict["offset"]["totalSeconds"]
            )
        )
    )


class WeblogHandler(BaseHTTPRequestHandler):
    "A handler for Nextflow's web log plugin."

    def log_message(self, format, *args):  # pylint: disable=redefined-builtin
        # The base class's implementation writes directly to stderr
        message = format % args
        logging.getLogger("WeblogHandler").debug(message)

    def log_error(self, format, *args):  # pylint: disable=redefined-builtin
        # The base class's implementation defers to log_message
        message = format % args
        logging.getLogger("WeblogHandler").warning(message)

    def do_POST(self):  # pylint: disable=invalid-name
        "Handle a POST."
        raw_content = self.rfile.read(int(self.headers["Content-Length"])).decode(
            "utf-8"
        )

        # Respond with 204 NO CONTENT, as that doesn't require a response body
        self.send_response(204)
        self.end_headers()

        # The absolute level doesn't matter, but it should be consistent
        logging.getLogger("trace_logger").info(raw_content)

        # Pass the trace to the server to (potentially) send an email
        self.server.send_email(raw_content)


class EmailingServer(HTTPServer):
    "An HTTPServer that sends out emails announcing pipeline status changes."
    EVENTS = {"started", "completed"}

    def __init__(self, *args, project_id: str, patient: str, email: str):
        super().__init__(*args)

        self.subject = f"Metapipeline: {project_id} ({patient})"
        self.email = email.strip()

        self.base_message_id = None
        self.logger = logging.getLogger("EmailingServer")

    def send_email(self, raw_content: bytes):
        "Send an email announcing when pipelines start or stop."
        if not self.email:
            # Don't bother doing anything with this
            return

        data = json.loads(raw_content)

        if data["event"] not in self.EVENTS:
            # Not a pipeline start or complete event
            return

        manifest_name = data["metadata"]["workflow"]["manifest"]["name"]
        project_name = data["metadata"]["workflow"]["projectName"]

        self.logger.debug(
            "manifest_name: %s, project_name: %s, event: %s",
            manifest_name,
            project_name,
            data["event"]
        )

        start_time = parse_time(data["metadata"]["workflow"]["start"])
        completed_time = parse_time(data["metadata"]["workflow"]["complete"])

        message_lines = [
            "",
            f"Manifest name: {manifest_name}",
            f"Project name: {project_name}",
            f"Run name: {data['runName']}",
            ""
            f"Start time: {start_time}"
        ]

        if data["event"] == "started":
            message_lines.insert(0, "Pipeline run started.")
        else:
            message_lines.append(f"Completed time: {completed_time}")

            if data["metadata"]["workflow"]["success"]:
                message_lines.insert(0, "Pipeline run successful!")

            else:
                message_lines.insert(0, "Pipeline run FAILED!")
                message_lines.extend([
                    "",
                    f"Exit status: {data['metadata']['workflow']['exitStatus']}",
                    f"Error message: {data['metadata']['workflow']['errorMessage']}",
                    f"Error report: {data['metadata']['workflow']['errorReport']}",
                ])

        msg = MIMEText("\n".join(message_lines))
        msg["To"] = self.email

        # Ideally all of these emails would be threaded together. Use the
        # Message-Id and In-Reply-To headers in an attempt to do that.
        # Outlook also requires that the subjects all be the same.
        if self.base_message_id is None:
            msg["Subject"] = self.subject
            # Construct a unique message ID
            self.base_message_id = \
                f"<{time.time()}@{socket.gethostname()}.localdomain>"
            msg.add_header("Message-Id", self.base_message_id)

        else:
            msg["Subject"] = "Re: " + self.subject
            msg.add_header("In-Reply-To", self.base_message_id)
            msg.add_header("References", self.base_message_id)

        try:
            subprocess.run(
                ["sendmail", "-t", "-oi"],
                input=msg.as_bytes(),
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                check=True
            )
        except subprocess.CalledProcessError as err:
            self.logger.exception("Failed to send email!")
            self.logger.debug("stdout: %s", err.stdout)
            self.logger.debug("stderr: %s", err.stderr)


def run():
    "Run the Nextflow pipeline with additional logging."
    # Parse the --metapipeline_log_output_dir argument
    parser = argparse.ArgumentParser()

    # These three are to construct the output log file
    parser.add_argument("--metapipeline_log_output_dir", required=True)
    parser.add_argument("--task_hash", required=True)
    parser.add_argument("--patient", required=True)

    # These two (plus --patient) are used to send pipeline update emails
    parser.add_argument("--project_id", required=True)
    parser.add_argument("--status_email_address", required=True)
    args, _ = parser.parse_known_args()

    # Write logs into the patient-specific subdirectory of nextflow-log
    log_output_dir = Path(
        args.metapipeline_log_output_dir,
        "nextflow-log",
        f"{args.patient}-{args.task_hash}"
    ).resolve()
    log_output_dir.mkdir(parents=True, exist_ok=True)

    # Configure logging from this script to go to a `server.log` file
    logging.basicConfig(
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        level=logging.DEBUG,
        filename=log_output_dir / "server.log",
    )

    main_logger = logging.getLogger(__name__)

    main_logger.info("Logging server starting up")

    # Configure the Nextflow weblogs to go to a `traces.jsonl` file, and
    # exclude them from server.log

    # Construct the log file handler
    handler = logging.FileHandler(Path(log_output_dir, "traces.jsonl"))
    # The absolute level doesn't matter, but it needs to be consistent
    handler.setLevel(logging.INFO)
    handler.setFormatter(logging.Formatter("%(message)s"))

    trace_logger = logging.getLogger("trace_logger")
    trace_logger.addHandler(handler)
    trace_logger.setLevel(logging.INFO)

    # Do not pass weblog messages to any higher handlers
    trace_logger.propagate = False

    with contextlib.ExitStack() as stack:
        log_server = stack.enter_context(EmailingServer(
            ("localhost", 0),
            WeblogHandler,
            project_id=args.project_id,
            patient=args.patient,
            email=args.status_email_address,
        ))

        # Ensure that the logging server shuts down after Nextflow returns
        stack.callback(main_logger.info, "Log server shut down")
        stack.callback(log_server.shutdown)

        # Start the logging server in another thread. It will die after the
        # ExitStack unwinds and log_server.shutdown is called
        threading.Thread(
            name="WeblogThread",
            target=log_server.serve_forever,
        ).start()
        main_logger.info("Logging thread started")

        # Reconstruct the arguments to this script
        nextflow_args = ["nextflow"]
        nextflow_args.extend(sys.argv[1:])

        # Add arguments to make Nextflow log to the server
        nextflow_args.extend([
            "-with-weblog",
            f"http://localhost:{log_server.server_address[1]}",
        ])

        # Start Nextflow in the same process group so that it receives the same
        # signals as this script
        stack.callback(main_logger.info, "Nextflow processes exited")
        nextflow_process = stack.enter_context(
            subprocess.Popen(nextflow_args, start_new_session=False)
        )

        # Ignore SIGTERM and SIGINT while Nextflow is still active to ensure
        # that we capture any last logs it emits after being killed.
        stack.enter_context(ignore_signals(signal.SIGTERM, signal.SIGINT))

        # Wait for and return Nextflow's exit code.
        nextflow_process.wait()
        return nextflow_process.returncode


if __name__ == "__main__":
    exit_code = run()
    logging.getLogger(__name__).info("Exiting with code %d", exit_code)
    sys.exit(exit_code)
