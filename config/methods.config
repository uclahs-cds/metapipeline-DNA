import groovy.util.ConfigSlurper
includeConfig "${projectDir}/external/pipeline-Nextflow-config/config/schema/schema.config"
includeConfig "${projectDir}/external/pipeline-Nextflow-config/config/csv/csv_parser.config"
includeConfig "${projectDir}/config/pipeline_selector.config"

def get_submodule_version(submodule) {
    def manifest_locations = [new File("${projectDir}/external/${submodule}/nextflow.config"), new File("${projectDir}/external/${submodule}/pipeline/nextflow.config")]
    def submodule_manifest = null

    for (a_manifest in manifest_locations) {
        if ( a_manifest.exists() ) {
            submodule_manifest = a_manifest
            break
        }
    }

    def version = 'null'

    if ( ! submodule_manifest ) {
        System.out.println(" ### WARNING ### Manifest for ${submodule} not found!")
        return 'null'
    }

    submodule_manifest.eachLine { line ->
        curr_line = line.replaceAll('\\s', '')
        if (curr_line in String && curr_line.startsWith('version')) {
            version = curr_line.split('=')[-1].replaceAll('\'', '').replaceAll('\"', '')
        }
    }

    return version
}

methods {
    set_output_dirs = {
        def tz = TimeZone.getTimeZone("UTC")
        def date = new Date().format("yyyyMMdd'T'HHmmss'Z'", tz)

        def base_output_dir = "${params.output_dir}/${manifest.name}-${manifest.version}/${params.project_id}"
        params.log_output_dir = "${base_output_dir}/log-${manifest.name}-${date}"
        params.final_output_dir = "${base_output_dir}/main_workflow"
    }

    set_pipeline_logs = {
        trace.enabled = true
        trace.file = "${params.log_output_dir}/nextflow-log/trace.txt"

        timeline.enabled = true
        timeline.file = "${params.log_output_dir}/nextflow-log/timeline.html"

        report.enabled = true
        report.file = "${params.log_output_dir}/nextflow-log/report.html"
    }

    set_pipeline_cpus = {
        serial_pipelines = ['call_gSNP', 'align_DNA']
        def cpus_and_memory = methods.detect_cpus_and_memory()
        if (!cpus_and_memory) {
            return
        }
        cpus = cpus_and_memory['cpus']
        serial_cpus = cpus
        parallel_cpus = (cpus < 8) ? cpus : 8

        params.pipeline_params.each { k, v ->
            v.subworkflow_cpus = (serial_pipelines.contains(k)) ? serial_cpus : parallel_cpus
        }
    }

    detect_cpus_and_memory = {
        if (params.partition ==~ /F\d*/) {
            def cpus = params.partition.replaceAll('F', '') as Integer
            // F-series 1:2 CPU:memory ratio
            def memory = 2 * cpus * 0.95 // 5% of memory is unavailable based on Slurm configuration
            return ['cpus': cpus, 'memory': memory]
        } else if (params.partition ==~ /M\d*/) {
            def cpus = params.partition.replaceAll('M', '') as Integer
            // M-series 1:16 CPU:memory ratio
            def memory = 16 * cpus * 0.95 // %5 of memory is unavailable based on Slurm configuration
            return ['cpus': cpus, 'memory': memory]
        } else {
            System.out.println("Failed to detect CPUs and memory for ${params.partition}. Using default values.")
            return [:]
        }
    }

    set_process = {
        def cpus_and_memory = methods.detect_cpus_and_memory()
        def allocation_string = (cpus_and_memory) ? "-c ${cpus_and_memory['cpus']} --mem ${cpus_and_memory['memory']}G" : ""
        if (params.uclahs_cds_wgs) {
            params.time_delay = Math.max(params.cluster_submission_interval, params.global_rate_limit)

            process['withName:call_metapipeline_DNA'].maxForks = 1
            def job_name_prefix = "call_metapipeline_DNA"

            params.global_job_submission_limiter = """
                sleep \$((RANDOM % 60))
                submit_signal="false"
                while [ \$submit_signal == "false" ]
                do
                    running_metapipeline_jobs=`squeue --noheader --format="%j" | grep ^${job_name_prefix} | wc -l`
                    running_user_metapipeline_jobs=\$(squeue --noheader --format="%j" -u `whoami` | grep ^${job_name_prefix} | wc -l)
                    if [ \$running_metapipeline_jobs -eq 0 ]
                    then
                        submit_signal="true"
                    elif [ \$running_metapipeline_jobs -ge ${params.global_allowed_jobs} ] || [ \$running_user_metapipeline_jobs -ge ${params.per_user_allowed_jobs} ]
                    then
                        sleep 60
                    else
                        last_submitted_job=`squeue --noheader --sort=-V --format="%j---%i---%T---%S" | grep ^${job_name_prefix} | head -n 1`
                        last_submitted_job_status=`echo \$last_submitted_job | awk -F"---" {'print \$3'}`
                        if [ \$last_submitted_job_status != "RUNNING" ]
                        then
                            sleep 60
                        else
                            last_submission_time=`echo \$last_submitted_job | awk -F"---" {'print \$4'}`
                            last_submission_time_seconds=`date -d \$last_submission_time +%s`
                            now_seconds=`date +%s`
                            wait_time_for_next_submission=\$((${params.time_delay} * 60 - (now_seconds - last_submission_time_seconds)))
                            if [ \$wait_time_for_next_submission -le 0 ]
                            then
                                submit_signal="true"
                            else
                                sleep \$wait_time_for_next_submission
                            fi
                        fi
                    fi
                done
            """
            params.global_job_submission_sbatch = """
                sbatch \
                    -o `pwd`/.command.log \
                    --no-requeue \
                    ${allocation_string} \
                    -p ${params.partition} \
                    ${params.clusterOptions} \
            """
        } else {
            if (cpus_and_memory) {
                process['withName:call_metapipeline_DNA'].cpus = cpus_and_memory['cpus']
                process['withName:call_metapipeline_DNA'].memory = "${cpus_and_memory['memory']}GB" as nextflow.util.MemoryUnit
            }
            process['withName:call_metapipeline_DNA'].executor = params.executor
            process['withName:call_metapipeline_DNA'].maxForks = params.max_parallel_jobs
            process['withName:call_metapipeline_DNA'].queue = params.partition
            process['withName:call_metapipeline_DNA'].clusterOptions = "${params.clusterOptions}"
            executor."\$${params.executor}".submitRateLimit = "1/${params.cluster_submission_interval}min"
            executor."\$${params.executor}".jobName = { ->
                "${task.process}_${task.hash.substring(0,2)}_${task.hash.substring(2)}"
            }
        }
    }

    set_submodule_versions = {
        params.version_BAM2FASTQ  = get_submodule_version('pipeline-convert-BAM2FASTQ')
        params.version_align_DNA  = get_submodule_version('pipeline-align-DNA')
        params.version_call_gSNP  = get_submodule_version('pipeline-call-gSNP')
        params.version_call_sSNV  = get_submodule_version('pipeline-call-sSNV')
        params.version_call_mtSNV = get_submodule_version('pipeline-call-mtSNV')
        params.version_call_gSV   = get_submodule_version('pipeline-call-gSV')
        params.version_call_sSV   = get_submodule_version('pipeline-call-sSV')
    }

    set_env = {
        // Ensure leading work dir is set to a directory shared between nodes, eg. /hot
        if (params.ucla_cds) {
            if (! params.leading_work_dir.startsWith('/hot')) {
                throw new Exception("The leading_work_dir must be set to a common directory across nodes (ie. in /hot for ucla_cds). Received ${params.leading_work_dir} instead.")
            }
        }

        schema.check_path(params.leading_work_dir, 'w')
        workDir = params.leading_work_dir

        // Only skip the check if ucla_cds is set to false. In all other cases, perform the check.
        if (params.ucla_cds) {
            if (! params.pipeline_work_dir.startsWith('/scratch')) {
                System.out.println("Pipeline_work_dir is being set to a directory other than /scratch! Please be very careful about I/O operations and network latency!")
            }
        }

        // Setting the work_dir parameter for individual pipelines
        if (params.ucla_cds) {
            /**
             * By default, set the work_dir parameter to /scratch when not specified.
             *
             * WARNING: changing this directory can lead to high server latency and
             * potential disk space limitations. Change with caution! The 'workDir'
             * in Nextflow determines the location of intermediate and temporary files.
             */
            params.work_dir = (params.containsKey("work_dir") && params.work_dir) ? params.work_dir : "/scratch"
            schema.check_path(params.work_dir, 'w')
        } else {
            // If work_dir was specified as a param and exists or can be created, set workDir. Otherwise, let Nextflow's default behavior dictate workDir
            if (params.containsKey("work_dir") && params.work_dir) {
                schema.check_path(params.work_dir, 'w')
            } else {
                // If not ucla_cds and no work_dir is set, leave it as an empty string and let individual pipelines handle workDir
                params.work_dir = ''
            }
        }
    }

    parse_input = {
        if (params.containsKey('input')) { // if params.input exists then YAML input is used
            if (params.input.size() > 1) {
                throw new Exception("More than one input is specified in the YAML, please choose one of BAM or CRAM or FASTQ")
            }
            if (params.input.containsKey('CRAM')) {
                params.input.BAM = params.input.CRAM
                params.input.remove('CRAM')
            }
        } else if (params.containsKey('input_csv')) { // check if CSV input is used
            reader = new BufferedReader(new FileReader(params.input_csv))
            header_line = reader.readLine().split(',') // reads first line of input csv file and split by comma into list
            if (header_line.contains('read1_fastq')) { // for FASTQ csv
                def fastq_input_fields = ['patient', 'sample', 'state', 'read_group_identifier', 'sequencing_center', 'library_identifier', 'platform_technology', 'platform_unit', 'bam_header_sm', 'lane', 'read1_fastq', 'read2_fastq']
                params.input.FASTQ = csv_parser.parse_csv(params.input_csv, fastq_input_fields)
            } else if (header_line.contains('path')) { // for BAM/CRAM csv
                def bam_input_fields = ['patient', 'sample', 'state', 'path']
                params.input.BAM = csv_parser.parse_csv(params.input_csv, bam_input_fields)
            } else {
                throw new Exception("input CSV does not follow the format for either BAM/CRAM or FASTQ inputs")
            }
        } else {
            throw new Exception("Neither YAML nor CSV inputs found! Please run pipeline with inputs.")
        }

        params.input_type = (params.input.containsKey('BAM')) ? 'BAM' : 'FASTQ'
    }

    enable_pipelines = { List pipelines ->
        def pipeline_name_map = [
            'convert_BAM2FASTQ': 'convert-BAM2FASTQ',
            'align_DNA': 'align-DNA',
            'call_gSNP': 'call-gSNP',
            'call_sSNV': 'call-sSNV',
            'call_mtSNV': 'call-mtSNV',
            'call_gSV': 'call-gSV',
            'call_sSV': 'call-sSV'
        ]

        params.pipeline_params.each { k, v ->
            v.is_pipeline_enabled = pipelines.contains(pipeline_name_map[k])
        }
    }

    resolve_pipeline_selection = {
        def pipelines_to_run = pipeline_selector.get_pipelines(params.requested_pipelines, params.input_type)
        if (params.input_type == 'FASTQ') {
            // Do not allow overriding alignment and call-gSNP with FASTQ input
            System.out.println('INFO - FASTQ input detected. Call-gSNP and realignment overriding will automatically be disabled.')
            params.override_realignment = false
            params.override_call_gsnp = false
            methods.enable_pipelines(pipelines_to_run)
            params.enable_input_deletion_call_gsnp = true
            return pipelines_to_run
        }
        def pipelines_to_remove = []
        if (params.override_call_gsnp) {
            pipelines_to_remove.add('call-gSNP')
            System.out.println('INFO - Call-gSNP override detected. Realignment will automatically be overriden.')
            params.override_realignment = true
        }
        if (params.override_realignment) {
            pipelines_to_remove.add('convert-BAM2FASTQ')
            pipelines_to_remove.add('align-DNA')
        }
        System.out.println("INFO - With the override options, ${pipelines_to_remove} will be skipped.")
        pipelines_to_run.removeAll { pipelines_to_remove.contains(it) }
        if (pipelines_to_run.isEmpty()) {
            throw new Exception("Current pipeline selection settings result in 0 pipelines being run. Please double-check settings.")
        }
        methods.enable_pipelines(pipelines_to_run)

        params.enable_input_deletion_call_gsnp = pipelines_to_run.contains('align-DNA')
    }

    parse_individual_pipeline_default_config = { String pipeline ->
        temp_file = File.createTempFile("meta-${pipeline}-temp", ".tmp")
        raw_config = new File("${projectDir}/module/${pipeline}/default.config")

        // Keep only the params block from the `default.config` for automatic parsing
        // Removes any import statements and also the algorithm parameters since they're handled via strings in the metapipeline
        raw_config.eachLine { line ->
            if (!line.startsWith('includeConfig') && !line.startsWith('methods.set') && !line.contains('algorithm =')) {
                temp_file.append("${line}\n")
            }
        }

        config_parser = new ConfigSlurper()
        return config_parser.parse(temp_file.toURL())
    }

    validate_pipelines = {
        pipeline_param_exclusion = [
            'pipeline-call-gSNP': ['dataset_id', 'patient_id', 'sample_id', 'output_dir', 'input_csv', 'input'],
            'pipeline-call-sSNV': ['dataset_id', 'patient_id', 'sample_id', 'output_dir', 'input'],
            'pipeline-call-sSV': ['dataset_id', 'patient_id', 'output_dir', 'input_csv']
        ]

        pipeline_param_exclusion.each { pipeline, to_exclude ->
            System.out.println("INFO - Validating ${pipeline} parameters...")
            pipeline_key = pipeline.replaceAll('pipeline-', '').replaceAll('-', '_')
            current_pipeline_params = methods.parse_individual_pipeline_default_config(pipeline_key).params + params.pipeline_params[pipeline_key]
            File custom_types = new File("${projectDir}/external/${pipeline}/config/custom_schema_types.config")
            if (custom_types.exists()) {
                schema.load_custom_types(custom_types.toString())
            }
            schema.validate_specific("${projectDir}/external/${pipeline}/config/schema.yaml", current_pipeline_params, to_exclude)
        }
    }

    clean_param_value = { raw_value ->
        def value_class = raw_value.getClass()
        if (value_class == String || value_class == GString) {
            return "\'${raw_value}\'"
        }

        return raw_value
    }

    generate_pipeline_arg_strings = {
        def params_exclude_from_all = ['output_dir', 'input_csv', 'work_dir', 'sample_id', 'patient_id', 'dataset_id', 'input']
        def specific_params_to_exclude = [
            'call_sSNV': ['algorithm'],
            'call_sSV': ['algorithm']
        ]

        params.pipeline_params.each { pipeline, pipeline_params ->
            def arg_str = ''
            def params_to_exclude = (specific_params_to_exclude.containsKey(pipeline)) ? specific_params_to_exclude[pipeline] + params_exclude_from_all : params_exclude_from_all
            pipeline_params.each { param_key, param_value ->
                if (!params_to_exclude.contains(param_key)) {
                    arg_str = "${arg_str} --${param_key} ${methods.clean_param_value(param_value)}"
                }
            }

            pipeline_params.metapipeline_arg_string = arg_str
        }
    }

    set_up = {
        methods.parse_input()
        schema.load_custom_types("${projectDir}/config/custom_schema_types.config")
        schema.validate()
        methods.set_output_dirs()
        methods.set_pipeline_logs()
        methods.set_process()
        methods.set_submodule_versions()
        methods.set_env()
        methods.resolve_pipeline_selection()
        methods.set_pipeline_cpus()
        methods.generate_pipeline_arg_strings()
        methods.validate_pipelines()
    }
}
