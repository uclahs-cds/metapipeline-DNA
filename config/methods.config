import groovy.util.ConfigSlurper
includeConfig "${projectDir}/external/pipeline-Nextflow-config/config/schema/schema.config"
includeConfig "${projectDir}/external/pipeline-Nextflow-config/config/csv/csv_parser.config"
includeConfig "${projectDir}/external/pipeline-Nextflow-config/config/methods/common_methods.config"
includeConfig "${projectDir}/config/pipeline_selector.config"

def get_submodule_version(submodule) {
    def manifest_locations = [new File("${projectDir}/external/${submodule}/nextflow.config"), new File("${projectDir}/external/${submodule}/pipeline/nextflow.config")]
    def submodule_manifest = null

    for (a_manifest in manifest_locations) {
        if ( a_manifest.exists() ) {
            submodule_manifest = a_manifest
            break
        }
    }

    def version = 'null'

    if ( ! submodule_manifest ) {
        System.out.println(" ### WARNING ### Manifest for ${submodule} not found!")
        return 'null'
    }

    submodule_manifest.eachLine { line ->
        curr_line = line.replaceAll('\\s', '')
        if (curr_line in String && curr_line.startsWith('version')) {
            version = curr_line.split('=')[-1].replaceAll('\'', '').replaceAll('\"', '')
        }
    }

    return version
}

methods {
    set_output_dirs = {
        def tz = TimeZone.getTimeZone("UTC")
        def date = new Date().format("yyyyMMdd'T'HHmmss'Z'", tz)

        def base_output_dir = "${params.output_dir}/${manifest.name}-${manifest.version}/${params.project_id}"
        params.log_output_dir = "${base_output_dir}/log-${manifest.name}-${date}"
        params.final_output_dir = "${base_output_dir}/main_workflow"
    }

    set_pipeline_logs = {
        trace.enabled = true
        trace.file = "${params.log_output_dir}/nextflow-log/trace.txt"

        timeline.enabled = true
        timeline.file = "${params.log_output_dir}/nextflow-log/timeline.html"

        report.enabled = true
        report.file = "${params.log_output_dir}/nextflow-log/report.html"
    }

    set_pipeline_cpus = {
        serial_pipelines = ['recalibrate_BAM', 'align_DNA', 'call_gSNP']
        def cpus_and_memory = methods.detect_cpus_and_memory()
        if (!cpus_and_memory) {
            return
        }
        cpus = cpus_and_memory['cpus']
        serial_cpus = cpus
        parallel_cpus = (cpus < 8) ? cpus : 8

        params.pipeline_params.each { k, v ->
            v.subworkflow_cpus = (serial_pipelines.contains(k)) ? serial_cpus : parallel_cpus
        }
    }

    detect_cpus_and_memory = {
        if (params.partition ==~ /F\d*/) {
            def cpus = params.partition.replaceAll('F', '') as Integer
            // F-series 1:2 CPU:memory ratio
            def memory = 2 * cpus * 0.95 // 5% of memory is unavailable based on Slurm configuration
            return ['cpus': cpus, 'memory': memory]
        } else if (params.partition ==~ /M\d*/) {
            def cpus = params.partition.replaceAll('M', '') as Integer
            // M-series 1:16 CPU:memory ratio
            def memory = 16 * cpus * 0.95 // %5 of memory is unavailable based on Slurm configuration
            return ['cpus': cpus, 'memory': memory]
        } else {
            System.out.println("Failed to detect CPUs and memory for ${params.partition}. Using default values.")
            return [:]
        }
    }

    set_process = {
        def cpus_and_memory = methods.detect_cpus_and_memory()
        def allocation_string = (cpus_and_memory) ? "-c ${cpus_and_memory['cpus']} --mem ${Math.floor(cpus_and_memory['memory'] * 1024) as Integer}M" : ""
        if (params.uclahs_cds_wgs) {
            params.time_delay = Math.max(params.cluster_submission_interval, params.global_rate_limit)

            process['withName:call_metapipeline_DNA'].maxForks = 1
            def job_name_prefix = "wgs_call_metapipeline_DNA"

            final Integer SECONDS_IN_ONE_MINUTE = 60

            params.global_job_submission_limiter = """
                hold_submission=true
                while [ "\$hold_submission" = true ]
                do
                    running_metapipeline_jobs=`squeue --noheader --sort=-V --format="%j---%i---%T---%S---%u" | grep ^${job_name_prefix} || true`
                    if [ -z "\$running_metapipeline_jobs" ]
                    then
                        running_metapipeline_jobs_number=0
                        running_user_metapipeline_jobs_number=0
                    else
                        running_metapipeline_jobs_number=`echo "\$running_metapipeline_jobs" | wc -l`
                        running_user_metapipeline_jobs_number=\$(echo "\$running_metapipeline_jobs" | awk -F"---" {'print \$5'} | grep "`whoami`" | wc -l)
                    fi
                    if [ \$running_metapipeline_jobs_number -eq 0 ]
                    then
                        hold_submission=false
                    elif [ \$running_metapipeline_jobs_number -ge ${params.global_allowed_jobs} ] || [ \$running_user_metapipeline_jobs_number -ge ${params.per_user_allowed_jobs} ]
                    then
                        sleep ${SECONDS_IN_ONE_MINUTE}
                    else
                        last_submitted_job=`echo "\$running_metapipeline_jobs" | head -n 1`
                        last_submitted_job_status=`echo \$last_submitted_job | awk -F"---" {'print \$3'}`
                        if [ \$last_submitted_job_status != "RUNNING" ]
                        then
                            sleep ${SECONDS_IN_ONE_MINUTE}
                        else
                            last_submission_time=`echo \$last_submitted_job | awk -F"---" {'print \$4'}`
                            last_submission_time_seconds=`date -d \$last_submission_time +%s`
                            now_seconds=`date +%s`
                            wait_time_for_next_submission=\$((${params.time_delay} * ${SECONDS_IN_ONE_MINUTE} - (now_seconds - last_submission_time_seconds)))
                            if [ \$wait_time_for_next_submission -le 0 ]
                            then
                                hold_submission=false
                            else
                                echo "Waiting to submit job until previously submitted job is running and sufficient time has passed since submission..."
                                sleep \$((\$wait_time_for_next_submission + (RANDOM % ${SECONDS_IN_ONE_MINUTE})))
                            fi
                        fi
                    fi
                done
                CURRENT_WORK_DIR=`pwd`
                FIRST_DIR_HASH=\$(basename `dirname \$CURRENT_WORK_DIR`)
                SECOND_DIR_HASH=\$(basename \$CURRENT_WORK_DIR)
            """
            params.global_job_submission_sbatch = """
                SBATCH_RET=\$(sbatch \
                    -o \$CURRENT_WORK_DIR/.command.log \
                    --no-requeue \
                    ${allocation_string} \
                    -p ${params.partition} \
                    ${params.clusterOptions} \
            """
        } else {
            if (cpus_and_memory) {
                process['withName:call_metapipeline_DNA'].cpus = cpus_and_memory['cpus']
                process['withName:call_metapipeline_DNA'].memory = "${cpus_and_memory['memory']}GB" as nextflow.util.MemoryUnit
            }
            process['withName:call_metapipeline_DNA'].executor = params.executor
            process['withName:call_metapipeline_DNA'].maxForks = params.max_parallel_jobs
            process['withName:call_metapipeline_DNA'].queue = params.partition
            process['withName:call_metapipeline_DNA'].clusterOptions = "${params.clusterOptions}"
            executor."\$${params.executor}".submitRateLimit = "1/${params.cluster_submission_interval}min"
            executor."\$${params.executor}".jobName = { ->
                "${task.process}_${task.hash.substring(0,2)}_${task.hash.substring(2)}"
            }
        }
    }

    set_submodule_versions = {
        params.version_BAM2FASTQ        = get_submodule_version('pipeline-convert-BAM2FASTQ')
        params.version_align_DNA        = get_submodule_version('pipeline-align-DNA')
        params.version_recalibrate_BAM  = get_submodule_version('pipeline-recalibrate-BAM')
        params.version_call_gSNP        = get_submodule_version('pipeline-call-gSNP')
        params.version_call_sSNV        = get_submodule_version('pipeline-call-sSNV')
        params.version_call_mtSNV       = get_submodule_version('pipeline-call-mtSNV')
        params.version_call_gSV         = get_submodule_version('pipeline-call-gSV')
        params.version_call_sSV         = get_submodule_version('pipeline-call-sSV')
    }

    set_env = {
        // Ensure leading work dir is set to a directory shared between nodes, eg. /hot
        if (params.ucla_cds) {
            if (! params.leading_work_dir.startsWith('/hot')) {
                throw new Exception("The leading_work_dir must be set to a common directory across nodes (ie. in /hot for ucla_cds). Received ${params.leading_work_dir} instead.")
            }
        }

        schema.check_path(params.leading_work_dir, 'w')
        workDir = params.leading_work_dir

        // Only skip the check if ucla_cds is set to false. In all other cases, perform the check.
        if (params.ucla_cds) {
            String requested_pipeline_work_dir = new File(params.pipeline_work_dir).getAbsolutePath()
            String default_work_dir_raw = (params.uclahs_cds_wgs) ? '/scratch/\\\$SLURM_JOB_ID' : '/scratch/\$SLURM_JOB_ID'
            String default_work_dir = new File(default_work_dir_raw).getAbsolutePath()
            params.resolved_work_dir = (default_work_dir.startsWith(requested_pipeline_work_dir)) ? default_work_dir : requested_pipeline_work_dir

            if (! params.resolved_work_dir.startsWith('/scratch')) {
                System.out.println("Pipeline work dir is being set to a directory other than /scratch! Please be very careful about I/O operations and network latency!")
            }
        } else {
            params.resolved_work_dir = params.pipeline_work_dir
        }
    }

    parse_input = {
        if (params.containsKey('input')) { // if params.input exists then YAML input is used
            if (params.input.size() > 1) {
                throw new Exception("More than one input is specified in the YAML, please choose one of BAM or CRAM or FASTQ")
            }
            if (params.input.containsKey('CRAM')) {
                params.input.BAM = params.input.CRAM
                params.input.remove('CRAM')
            }
        } else if (params.containsKey('input_csv')) { // check if CSV input is used
            reader = new BufferedReader(new FileReader(params.input_csv))
            header_line = reader.readLine().split(',') // reads first line of input csv file and split by comma into list
            if (header_line.contains('read1_fastq')) { // for FASTQ csv
                def fastq_input_fields = ['patient', 'sample', 'state', 'read_group_identifier', 'sequencing_center', 'library_identifier', 'platform_technology', 'platform_unit', 'bam_header_sm', 'lane', 'read1_fastq', 'read2_fastq']
                params.input.FASTQ = csv_parser.parse_csv(params.input_csv, fastq_input_fields)
                if (!params.input.FASTQ.every { input_line -> input_line.sample == input_line.bam_header_sm }) {
                    throw new Exception("Input FASTQs must have matching sample and bam_hedaer_sm!")
                }
            } else if (header_line.contains('path')) { // for BAM/CRAM csv
                def bam_input_fields = ['patient', 'sample', 'state', 'path']
                params.input.BAM = csv_parser.parse_csv(params.input_csv, bam_input_fields)
            } else {
                throw new Exception("input CSV does not follow the format for either BAM/CRAM or FASTQ inputs")
            }
        } else {
            throw new Exception("Neither YAML nor CSV inputs found! Please run pipeline with inputs.")
        }

        params.input_type = (params.input.containsKey('BAM')) ? 'BAM' : 'FASTQ'
    }

    set_sample_counts = {
        Map sample_counts = [:]
        params.input[params.input_type].collect{ ['patient': it['patient'], 'sample': it['sample'], 'state': it['state']] }.unique().collect{ sample_input ->
            if (!sample_counts.containsKey(sample_input.patient)) {
                sample_counts[sample_input.patient] = ['normal': 0, 'tumor': 0]
            }

            sample_counts[sample_input.patient][sample_input.state] += 1
        }

        params.sample_counts = sample_counts

        if (params.sample_mode != 'single') {
            if (!params.sample_counts.every { patient, counts -> counts.normal == 1 }) {
                throw new Exception("Patients with multiple normal samples or no normal sample found! Please run the metapipeline with a single normal sample per patient.")
            }
        }
    }

    enable_pipelines = { List pipelines ->
        def pipeline_name_map = [
            'convert_BAM2FASTQ': 'convert-BAM2FASTQ',
            'align_DNA': 'align-DNA',
            'recalibrate_BAM': 'recalibrate-BAM',
            'call_gSNP': 'call-gSNP',
            'call_sSNV': 'call-sSNV',
            'call_mtSNV': 'call-mtSNV',
            'call_gSV': 'call-gSV',
            'call_sSV': 'call-sSV'
        ]

        params.pipeline_params.each { k, v ->
            v.is_pipeline_enabled = pipelines.contains(pipeline_name_map[k])
        }
    }

    resolve_pipeline_selection = {
        def pipelines_to_run = pipeline_selector.get_pipelines(params.requested_pipelines, params.input_type)
        if (params.input_type == 'FASTQ') {
            // Do not allow overriding alignment and recalibrate-BAM with FASTQ input
            System.out.println('INFO - FASTQ input detected. Recalibrate-BAM and realignment overriding will automatically be disabled.')
            params.override_realignment = false
            params.override_recalibrate_bam = false
            methods.enable_pipelines(pipelines_to_run)
            params.enable_input_deletion_recalibrate_bam = true
            return pipelines_to_run
        }
        def pipelines_to_remove = []
        if (params.override_recalibrate_bam) {
            pipelines_to_remove.add('recalibrate-BAM')
            System.out.println('INFO - Recalibrate-BAM override detected. Realignment will automatically be overriden.')
            params.override_realignment = true
        }
        if (params.override_realignment) {
            pipelines_to_remove.add('convert-BAM2FASTQ')
            pipelines_to_remove.add('align-DNA')
        }
        System.out.println("INFO - With the override options, ${pipelines_to_remove} will be skipped.")
        pipelines_to_run.removeAll { pipelines_to_remove.contains(it) }
        if (pipelines_to_run.isEmpty()) {
            throw new Exception("Current pipeline selection settings result in 0 pipelines being run. Please double-check settings.")
        }
        methods.enable_pipelines(pipelines_to_run)

        params.enable_input_deletion_recalibrate_bam = pipelines_to_run.contains('align-DNA')
    }

    parse_individual_pipeline_default_config = { String pipeline ->
        temp_file = File.createTempFile("meta-${pipeline}-temp", ".tmp")
        raw_config = new File("${projectDir}/module/${pipeline}/default.config")

        // Keep only the params block from the `default.config` for automatic parsing
        // Removes any import statements and also the algorithm parameters since they're handled via strings in the metapipeline
        raw_config.eachLine { line ->
            if (!line.startsWith('includeConfig') && !line.startsWith('methods.set') && !line.contains('algorithm =')) {
                temp_file.append("${line}\n")
            }
        }

        config_parser = new ConfigSlurper()
        return config_parser.parse(temp_file.toURL())
    }

    validate_pipelines = {
        pipeline_param_exclusion = [
            'pipeline-recalibrate-BAM': ['dataset_id', 'patient_id', 'output_dir', 'input'],
            'pipeline-call-gSNP': ['dataset_id', 'patient_id', 'output_dir', 'input'],
            'pipeline-call-sSNV': ['dataset_id', 'patient_id', 'sample_id', 'output_dir', 'work_dir', 'input'],
            'pipeline-call-sSV': ['dataset_id', 'patient_id', 'sample_id', 'output_dir', 'input']
        ]

        pipeline_param_exclusion.each { pipeline, to_exclude ->
            System.out.println("INFO - Validating ${pipeline} parameters...")
            pipeline_key = pipeline.replaceAll('pipeline-', '').replaceAll('-', '_')
            current_pipeline_params = methods.parse_individual_pipeline_default_config(pipeline_key).params + params.pipeline_params[pipeline_key]
            File custom_types = new File("${projectDir}/external/${pipeline}/config/custom_schema_types.config")
            if (custom_types.exists()) {
                schema.load_custom_types(custom_types.toString())
            }
            schema.validate_specific("${projectDir}/external/${pipeline}/config/schema.yaml", current_pipeline_params, to_exclude)
        }
    }

    clean_param_value = { raw_value ->
        def value_class = raw_value.getClass()
        if (value_class == String || value_class == GString) {
            return "\'${raw_value}\'"
        }

        return raw_value
    }

    is_empty_string = { raw_value ->
        def value_class = raw_value.getClass()
        if (value_class == String || value_class == GString) {
            return raw_value.isEmpty()
        }

        return false
    }

    generate_pipeline_arg_strings = {
        def params_exclude_from_all = ['output_dir', 'input_csv', 'work_dir', 'sample_id', 'patient_id', 'dataset_id', 'input']
        def specific_params_to_exclude = [
            'call_sSNV': ['algorithm']
        ]

        params.pipeline_params.each { pipeline, pipeline_params ->
            Map arg_map = [:]
            def params_to_exclude = (specific_params_to_exclude.containsKey(pipeline)) ? specific_params_to_exclude[pipeline] + params_exclude_from_all : params_exclude_from_all
            pipeline_params.each { param_key, param_value ->
                if (!params_to_exclude.contains(param_key) && !methods.is_empty_string(param_value)) {
                    arg_map[param_key] = param_value
                }
            }

            pipeline_params.metapipeline_arg_map = arg_map
        }
    }

    get_default_data_map = {
        Map default_data_map = [
            'align-DNA': ['BWA-MEM2': ['BAM':''], 'HISAT2': ['BAM':'']],
            'recalibrate-BAM': ['BAM':'', 'contamination_table':''],
            'convert-BAM2FASTQ': [
                    'read_group_identifier':'',
                    'sequencing_center':'',
                    'library_identifier':'',
                    'platform_techology':'',
                    'platform_unit':'',
                    'lane':'',
                    'read1_fastq':'',
                    'read2_fastq':''
                ],
            'original_data': [:]
        ]
    }

    generate_sample_data_map = {
        Map sample_data = [:]
        List original_data_keys = (params.input_type == 'BAM') ?
            ['path'] :
            [
                'read_group_identifier',
                'sequencing_center',
                'library_identifier',
                'platform_techology',
                'platform_unit',
                'bam_header_sm',
                'lane',
                'read1_fastq',
                'read2_fastq'
            ]
        params.input[params.input_type].each { sample_map ->
            String patient = sample_map['patient']
            String sample_id = sample_map['sample']
            String sample_state = sample_map['state']

            Map original_data = [:]
            original_data_keys.each{ data_key -> original_data[data_key] = sample_map[data_key] }

            Map default_data = methods.get_default_data_map()
            default_data['original_data'] = original_data

            sample_data[sample_id] = default_data + [
                'patient': patient,
                'sample_id': sample_id,
                'state': sample_state
            ]
        }

        params.sample_data = sample_data
    }

    set_up = {
        methods.parse_input()
        methods.set_sample_counts()
        schema.load_custom_types("${projectDir}/config/custom_schema_types.config")
        schema.validate()
        methods.set_output_dirs()
        methods.set_pipeline_logs()
        methods.set_process()
        methods.set_submodule_versions()
        methods.set_env()
        methods.resolve_pipeline_selection()
        methods.set_pipeline_cpus()
        methods.generate_pipeline_arg_strings()
        methods.validate_pipelines()
        methods.generate_sample_data_map()
    }
}
