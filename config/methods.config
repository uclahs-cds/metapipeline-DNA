import groovy.util.ConfigSlurper
includeConfig "${projectDir}/external/pipeline-Nextflow-config/config/schema/schema.config"
includeConfig "${projectDir}/external/pipeline-Nextflow-config/config/methods/common_methods.config"
includeConfig "${projectDir}/config/pipeline_selector.config"
includeConfig "${projectDir}/config/input_handler.config"

import nextflow.Nextflow

def get_submodule_version(submodule) {
    def manifest_locations = [new File("${projectDir}/external/${submodule}/nextflow.config"), new File("${projectDir}/external/${submodule}/pipeline/nextflow.config")]
    def submodule_manifest = null

    for (a_manifest in manifest_locations) {
        if ( a_manifest.exists() ) {
            submodule_manifest = a_manifest
            break
        }
    }

    def version = 'null'

    if ( ! submodule_manifest ) {
        System.out.println(" ### WARNING ### Manifest for ${submodule} not found!")
        return 'null'
    }

    submodule_manifest.eachLine { line ->
        curr_line = line.replaceAll('\\s', '')
        if (curr_line in String && curr_line.startsWith('version')) {
            version = curr_line.split('=')[-1].replaceAll('\'', '').replaceAll('\"', '')
        }
    }

    return version
}

methods {
    set_output_dirs = {
        def tz = TimeZone.getTimeZone("UTC")
        def date = new Date().format("yyyyMMdd'T'HHmmss'Z'", tz)

        def base_output_dir = "${params.output_dir}/${manifest.name}-${manifest.version}/${params.project_id}"
        params.log_output_dir = "${base_output_dir}/log-${manifest.name}-${date}"
        params.final_output_dir = "${base_output_dir}/main_workflow"
    }

    set_pipeline_logs = {
        trace.enabled = true
        trace.file = "${params.log_output_dir}/nextflow-log/trace.txt"

        timeline.enabled = true
        timeline.file = "${params.log_output_dir}/nextflow-log/timeline.html"

        report.enabled = true
        report.file = "${params.log_output_dir}/nextflow-log/report.html"
    }

    set_pipeline_cpus = {
        serial_pipelines = ['recalibrate_BAM', 'align_DNA', 'call_gSNP']
        def cpus_and_memory = methods.detect_cpus_and_memory()
        if (!cpus_and_memory) {
            return
        }
        cpus = cpus_and_memory['cpus']
        serial_cpus = cpus
        parallel_cpus = (cpus < 8 || params.run_downstream_pipelines_serially) ? cpus : 8

        params.pipeline_params.each { k, v ->
            v.subworkflow_cpus = (serial_pipelines.contains(k)) ? serial_cpus : parallel_cpus
        }
    }

    detect_cpus_and_memory = {
        if (params.partition ==~ /F\d*/) {
            def cpus = params.partition.replaceAll('F', '') as Integer
            // F-series 1:2 CPU:memory ratio
            def memory = 2 * cpus * 0.95 // 5% of memory is unavailable based on Slurm configuration
            return ['cpus': cpus, 'memory': memory]
        } else if (params.partition ==~ /M\d*/) {
            def cpus = params.partition.replaceAll('M', '') as Integer
            // M-series 1:16 CPU:memory ratio
            def memory = 16 * cpus * 0.95 // %5 of memory is unavailable based on Slurm configuration
            return ['cpus': cpus, 'memory': memory]
        } else {
            System.out.println("Failed to detect CPUs and memory for ${params.partition}. Using default values.")
            return [:]
        }
    }

    set_process = {
        def cpus_and_memory = methods.detect_cpus_and_memory()
        def allocation_string = (cpus_and_memory) ? "-c ${cpus_and_memory['cpus']} --mem ${Math.floor(cpus_and_memory['memory'] * 1024) as Integer}M" : ""
        if (params.uclahs_cds_wgs) {
            params.time_delay = Math.max(params.cluster_submission_interval, params.global_rate_limit)

            process['withName:call_metapipeline_DNA'].maxForks = 1
            def job_name_prefix = "wgs_call_metapipeline_DNA"

            final Integer SECONDS_IN_ONE_MINUTE = 60

            final String LIMITS_FILE = "/hot/software/pipeline/metapipeline-DNA/Nextflow/release/LIMITS.txt"

            final Integer DEFAULT_GLOBAL = 15
            final Integer DEFAULT_PER_USER = 2

            params.global_job_submission_limiter = """
                get_global_limit () {
                    local num_global=\$(grep -oP "^GLOBAL_LIMIT=\\K\\d+\$" ${LIMITS_FILE})

                    : \${num_global:=${DEFAULT_GLOBAL}}

                    echo \$num_global
                }

                get_user_limit () {
                    local user_limit=\$(grep -oP "^USER_LIMIT_`id -un`=\\K\\d+\$" ${LIMITS_FILE})

                    : \${user_limit:=${DEFAULT_PER_USER}}

                    echo \$user_limit
                }

                hold_submission=true
                while [ "\$hold_submission" = true ]
                do
                    running_jobs=`squeue --noheader --sort=-V --format="%j---%i---%T---%S---%u" || echo "failed"`
                    if [[ "\$running_jobs" == "failed" ]]
                    then
                        echo "squeue command failed, setting to limits to prevent submission"
                        running_metapipeline_jobs_number=`get_global_limit`
                        running_user_metapipeline_jobs_number=`get_user_limit`
                    else
                        if echo "\$running_jobs" | grep -q ^${job_name_prefix}
                        then
                            running_metapipeline_jobs_number=`echo "\$running_jobs" | grep ^${job_name_prefix} | wc -l`
                            running_user_metapipeline_jobs_number=\$(echo "\$running_jobs" | grep ^${job_name_prefix} | awk -F"---" {'print \$5'} | grep "`whoami`" | wc -l)
                        else
                            running_metapipeline_jobs_number=0
                            running_user_metapipeline_jobs_number=0
                        fi
                    fi

                    if [ \$running_metapipeline_jobs_number -eq 0 ]
                    then
                        hold_submission=false
                    elif [ \$running_metapipeline_jobs_number -ge `get_global_limit` ] || [ \$running_user_metapipeline_jobs_number -ge `get_user_limit` ]
                    then
                        sleep ${SECONDS_IN_ONE_MINUTE}
                    else
                        last_submitted_job=`echo "\$running_jobs" | grep ^${job_name_prefix} | head -n 1`
                        last_submitted_job_status=`echo \$last_submitted_job | awk -F"---" {'print \$3'}`
                        if [ \$last_submitted_job_status != "RUNNING" ]
                        then
                            sleep ${SECONDS_IN_ONE_MINUTE}
                        else
                            last_submission_time=`echo \$last_submitted_job | awk -F"---" {'print \$4'}`
                            last_submission_time_seconds=`date -d \$last_submission_time +%s`
                            now_seconds=`date +%s`
                            wait_time_for_next_submission=\$((${params.time_delay} * ${SECONDS_IN_ONE_MINUTE} - (now_seconds - last_submission_time_seconds)))
                            if [ \$wait_time_for_next_submission -le 0 ]
                            then
                                hold_submission=false
                            else
                                echo "Waiting to submit job until previously submitted job is running and sufficient time has passed since submission..."
                                sleep \$((\$wait_time_for_next_submission + (RANDOM % ${SECONDS_IN_ONE_MINUTE})))
                            fi
                        fi
                    fi
                done
                CURRENT_WORK_DIR=`pwd`
                FIRST_DIR_HASH=\$(basename `dirname \$CURRENT_WORK_DIR`)
                SECOND_DIR_HASH=\$(basename \$CURRENT_WORK_DIR)
            """
            params.global_job_submission_sbatch = """
                SBATCH_RET=\$(sbatch \
                    -o \$CURRENT_WORK_DIR/.command.log \
                    --no-requeue \
                    ${allocation_string} \
                    -p ${params.partition} \
                    ${params.clusterOptions} \
            """
        } else {
            if (cpus_and_memory) {
                process['withName:call_metapipeline_DNA'].cpus = cpus_and_memory['cpus']
                process['withName:call_metapipeline_DNA'].memory = "${cpus_and_memory['memory']}GB" as nextflow.util.MemoryUnit
            }
            process['withName:call_metapipeline_DNA'].executor = params.executor
            process['withName:call_metapipeline_DNA'].maxForks = params.max_parallel_jobs
            process['withName:call_metapipeline_DNA'].queue = params.partition
            process['withName:call_metapipeline_DNA'].clusterOptions = "${params.clusterOptions}"
            executor."\$${params.executor}".submitRateLimit = "1/${params.cluster_submission_interval}min"
            executor."\$${params.executor}".jobName = { ->
                "${task.process}_${task.hash.substring(0,2)}_${task.hash.substring(2)}"
            }
        }
    }

    set_submodule_versions = {
        params.version_BAM2FASTQ        = get_submodule_version('pipeline-convert-BAM2FASTQ')
        params.version_align_DNA        = get_submodule_version('pipeline-align-DNA')
        params.version_calculate_targeted_coverage = get_submodule_version('pipeline-calculate-targeted-coverage')
        params.version_recalibrate_BAM  = get_submodule_version('pipeline-recalibrate-BAM')
        params.version_call_gSNP        = get_submodule_version('pipeline-call-gSNP')
        params.version_call_sSNV        = get_submodule_version('pipeline-call-sSNV')
        params.version_call_mtSNV       = get_submodule_version('pipeline-call-mtSNV')
        params.version_call_gSV         = get_submodule_version('pipeline-call-gSV')
        params.version_call_sSV         = get_submodule_version('pipeline-call-sSV')
        params.version_call_sCNA        = get_submodule_version('pipeline-call-sCNA')
        params.version_generate_SQC_BAM = get_submodule_version('pipeline-generate-SQC-BAM')
        params.version_call_SRC         = get_submodule_version('pipeline-call-SRC')
        params.version_StableLift       = get_submodule_version('pipeline-StableLift')
        params.version_annotate_VCF     = get_submodule_version('pipeline-annotate-VCF')
        }

    set_env = {
        // Ensure leading work dir is set to a directory shared between nodes, eg. /hot
        if (params.ucla_cds) {
            if (! params.leading_work_dir.startsWith('/hot')) {
                throw new Exception("The leading_work_dir must be set to a common directory across nodes (ie. in /hot for ucla_cds). Received ${params.leading_work_dir} instead.")
            }
        }

        schema.check_path(params.leading_work_dir, 'w')
        workDir = params.leading_work_dir

        // Only skip the check if ucla_cds is set to false. In all other cases, perform the check.
        if (params.ucla_cds) {
            String requested_pipeline_work_dir = new File(params.pipeline_work_dir).getAbsolutePath()
            String default_work_dir_raw = (params.uclahs_cds_wgs) ? '/scratch/\\\$SLURM_JOB_ID' : '/scratch/\$SLURM_JOB_ID'
            String default_work_dir = new File(default_work_dir_raw).getAbsolutePath()
            params.resolved_work_dir = (default_work_dir.startsWith(requested_pipeline_work_dir)) ? default_work_dir : requested_pipeline_work_dir

            if (! params.resolved_work_dir.startsWith('/scratch')) {
                System.out.println("Pipeline work dir is being set to a directory other than /scratch! Please be very careful about I/O operations and network latency!")
            }
        } else {
            params.resolved_work_dir = params.pipeline_work_dir
        }
    }

    parse_individual_pipeline_default_config = { String pipeline ->
        temp_file = File.createTempFile("meta-${pipeline}-temp", ".tmp")
        raw_config = new File("${projectDir}/module/${pipeline}/default.config")

        // Keep only the params block from the `default.config` for automatic parsing
        // Removes any import statements and also the algorithm parameters since they're handled via strings in the metapipeline
        def keep_copying = true;
        raw_config.eachLine { line ->
            if (line.startsWith('methods.setup')) {
                keep_copying = false;
            }
            if (!line.startsWith('includeConfig') && !line.startsWith('methods.set') && !line.contains('algorithm =') && keep_copying) {
                temp_file.append("${line}\n")
            }
        }

        config_parser = new ConfigSlurper()
        return config_parser.parse(temp_file.toURL())
    }

    validate_pipelines = {
        pipeline_param_exclusion = [
            'pipeline-align-DNA': ['dataset_id', 'patient_id', 'sample_id', 'output_dir', 'work_dir', 'spark_temp_dir', 'input', 'input_csv'],
            'pipeline-recalibrate-BAM': ['dataset_id', 'patient_id', 'output_dir', 'input'],
            'pipeline-call-gSNP': ['dataset_id', 'patient_id', 'output_dir', 'input'],
            'pipeline-call-sSNV': ['dataset_id', 'patient_id', 'sample_id', 'output_dir', 'work_dir', 'input'],
            'pipeline-call-sSV': ['dataset_id', 'patient_id', 'sample_id', 'output_dir', 'input'],
            'pipeline-call-sCNA': ['dataset_id', 'patient_id', 'sample_id', 'output_dir', 'input'],
            'pipeline-generate-SQC-BAM': ['dataset_id', 'patient_id', 'sample_id', 'output_dir', 'input'],
            'pipeline-calculate-targeted-coverage': ['dataset_id', 'patient_id', 'sample_id', 'output_dir', 'input'],
            'pipeline-call-SRC': ['dataset_id', 'patient_id', 'sample_id', 'output_dir', 'input'],
            'pipeline-StableLift': ['sample_id', 'output_dir', 'input', 'rf_model', 'variant_caller'],
            'pipeline-annotate-VCF': ['dataset_id', 'patient_id', 'sample_id', 'output_dir', 'work_dir', 'input']
        ]

        pipeline_param_exclusion.each { pipeline, to_exclude ->
            System.out.println("INFO - Attempting to validate ${pipeline} parameters...")
            pipeline_key = pipeline.replaceAll('pipeline-', '').replaceAll('-', '_')
            if (!params.pipeline_params[pipeline_key].is_pipeline_enabled) {
                System.out.println("INFO - ${pipeline} is not enabled, skipping validation...")
                return
            }
            def current_pipeline_params = methods.parse_individual_pipeline_default_config(pipeline_key).params + params.pipeline_params[pipeline_key]
            File custom_types = new File("${projectDir}/external/${pipeline}/config/custom_schema_types.config")
            if (custom_types.exists()) {
                schema.load_custom_types(custom_types.toString())
            }
            schema.validate_specific("${projectDir}/external/${pipeline}/config/schema.yaml", current_pipeline_params, to_exclude)
            System.out.println("INFO - Validated ${pipeline} params")
        }
    }

    clean_param_value = { raw_value ->
        def value_class = raw_value.getClass()
        if (value_class == String || value_class == GString) {
            return "\'${raw_value}\'"
        }

        return raw_value
    }

    is_empty_string = { raw_value ->
        def value_class = raw_value.getClass()
        if (value_class == String || value_class == GString) {
            return raw_value.isEmpty()
        }

        return false
    }

    generate_pipeline_arg_strings = {
        def params_exclude_from_all = ['output_dir', 'input_csv', 'work_dir', 'sample_id', 'patient_id', 'dataset_id', 'input']
        def specific_params_to_exclude = [
            'call_sSNV': ['algorithm'],
            'StableLift': [
                'src_fasta_id',
                'src_fasta_ref',
                'src_fasta_fai',
                'src_fasta_dict',
                'dest_fasta_id',
                'dest_fasta_ref',
                'dest_fasta_fai',
                'dest_fasta_dict',
                'chain_file',
                'repeat_bed',
                'header_contigs',
                'gnomad_rds'
            ],
            'annotate_VCF': ['algorithm']
        ]
        def specific_params_allowed_empty = [
            'calculate_targeted_coverage': ['bait_bed', 'target_interval_list', 'bait_interval_list']
        ]

        params.pipeline_params.each { pipeline, pipeline_params ->
            Map arg_map = [:]
            def params_to_exclude = (specific_params_to_exclude.containsKey(pipeline)) ? specific_params_to_exclude[pipeline] + params_exclude_from_all : params_exclude_from_all
            def empty_params_to_include = (specific_params_allowed_empty.containsKey(pipeline)) ? specific_params_allowed_empty[pipeline] : []
            pipeline_params.each { param_key, param_value ->
                if (!params_to_exclude.contains(param_key) && (!methods.is_empty_string(param_value) || empty_params_to_include.contains(param_key))) {
                    arg_map[param_key] = param_value
                }
            }

            pipeline_params.metapipeline_arg_map = arg_map
        }
    }

    generate_pipeline_interval_params = {
        params.pipeline_interval_params = [
            'call_sSNV': 'intersect_regions',
            'call_gSNP': 'intervals',
            'recalibrate_BAM': 'intervals'
        ]
    }

    validate_metapipeline_params = {
        schema.load_custom_types("${projectDir}/config/custom_schema_types.config", true);
        def metapipeline_schema = schema.load_schema("${projectDir}/config/schema.yaml");
        def pipeline_params_schema = ['pipeline_params': metapipeline_schema.pipeline_params];
        pipeline_params_schema.elements.each { key, value ->
            value.required = (params.pipeline_params.containsKey(key) && params.pipeline_params[key].is_pipeline_enabled);
        }

        schema.validate_specific(pipeline_params_schema, params, []);
    }

    expand_stablelift_params = {
        // Make sure the user didn't set any of the advanced parameters
        def advanced_parameters = [
            'src_fasta_id',
            'src_fasta_ref',
            'src_fasta_fai',
            'src_fasta_dict',
            'dest_fasta_id',
            'dest_fasta_ref',
            'dest_fasta_fai',
            'dest_fasta_dict',
            'chain_file',
            'repeat_bed',
            'header_contigs',
            'gnomad_rds'
        ]

        for (key in advanced_parameters) {
            if (params.pipeline_params["StableLift"].containsKey(key)) {
                throw new Exception("Do not directly set params.${key} - the value will be inferred from params.liftover_direction")
            }
        }

        def liftover_direction  = params.pipeline_params["StableLift"].getOrDefault('liftover_direction', null)

        def forward  = "GRCh37ToGRCh38"
        def backward = "GRCh38ToGRCh37"

        if (liftover_direction in [forward, backward]) {
            if (liftover_direction == forward) {
                params.pipeline_params["StableLift"].src_fasta_id = 'GRCh37'
                params.pipeline_params["StableLift"].src_fasta_ref = params.pipeline_params["StableLift"].fasta_ref_37

                params.pipeline_params["StableLift"].dest_fasta_id = 'GRCh38'
                params.pipeline_params["StableLift"].dest_fasta_ref = params.pipeline_params["StableLift"].fasta_ref_38

                params.pipeline_params["StableLift"].chain_file = params.pipeline_params["StableLift"].resource_bundle_path + "/hg19ToHg38.over.chain"
                params.pipeline_params["StableLift"].repeat_bed = params.pipeline_params["StableLift"].resource_bundle_path + "/GRCh38_RepeatMasker-intervals.bed"
                params.pipeline_params["StableLift"].header_contigs = params.pipeline_params["StableLift"].resource_bundle_path + "/GRCh38_VCF-header-contigs.txt"
            } else {
                params.pipeline_params["StableLift"].src_fasta_id = 'GRCh38'
                params.pipeline_params["StableLift"].src_fasta_ref = params.pipeline_params["StableLift"].fasta_ref_38

                params.pipeline_params["StableLift"].dest_fasta_id = 'GRCh37'
                params.pipeline_params["StableLift"].dest_fasta_ref = params.pipeline_params["StableLift"].fasta_ref_37

                params.pipeline_params["StableLift"].chain_file = params.pipeline_params["StableLift"].resource_bundle_path + "/hg38ToHg19.over.chain"
                params.pipeline_params["StableLift"].repeat_bed = params.pipeline_params["StableLift"].resource_bundle_path + "/GRCh37_RepeatMasker-intervals.bed"
                params.pipeline_params["StableLift"].header_contigs = params.pipeline_params["StableLift"].resource_bundle_path + "/GRCh37_VCF-header-contigs.txt"
            }

            params.pipeline_params["StableLift"].src_fasta_fai = params.pipeline_params["StableLift"].src_fasta_ref + ".fai"
            params.pipeline_params["StableLift"].dest_fasta_fai = params.pipeline_params["StableLift"].dest_fasta_ref + ".fai"

            params.pipeline_params["StableLift"].src_fasta_dict = Nextflow.file(params.pipeline_params["StableLift"].src_fasta_ref).resolveSibling(Nextflow.file(params.pipeline_params["StableLift"].src_fasta_ref).getBaseName() + '.dict').toString()
            params.pipeline_params["StableLift"].dest_fasta_dict = Nextflow.file(params.pipeline_params["StableLift"].dest_fasta_ref).resolveSibling(Nextflow.file(params.pipeline_params["StableLift"].dest_fasta_ref).getBaseName() + '.dict').toString()

            params.pipeline_params["StableLift"].gnomad_rds = params.pipeline_params["StableLift"].resource_bundle_path + "/gnomad.v4.0.sv.Rds"
        }
    }

    set_up = {
        input_handler.convert_csv_inputs()
        schema.load_custom_types("${projectDir}/config/custom_schema_types.config")
        schema.validate_specific("${projectDir}/config/schema.yaml", params, ['pipeline_params'])
        input_handler.handle_inputs()
        methods.set_output_dirs()
        methods.set_pipeline_logs()
        methods.set_process()
        methods.set_submodule_versions()
        methods.set_env()
        pipeline_selector.handle_pipeline_selection()
        if (params.pipeline_params["StableLift"].is_pipeline_enabled) {
            methods.expand_stablelift_params()
        }
        methods.generate_pipeline_interval_params()
        methods.set_pipeline_cpus()
        methods.generate_pipeline_arg_strings()
        methods.validate_pipelines()
        methods.validate_metapipeline_params()
    }
}
