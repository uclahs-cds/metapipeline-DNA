includeConfig "${projectDir}/external/pipeline-Nextflow-config/config/schema/schema.config"
includeConfig "${projectDir}/external/pipeline-Nextflow-config/config/csv/csv_parser.config"

def get_submodule_version(submodule) {
    def submodule_dir = new File("${projectDir}/external/${submodule}")
    def commit_id = "git rev-parse HEAD".execute(null, submodule_dir).text.readLines()[0]
    return "git describe --tags ${commit_id}".execute(null, submodule_dir).text.readLines()[0]
}

// define reader to read csv header
def reader = new BufferedReader(new FileReader(params.input_csv))

methods {
    set_output_dirs = {
        def tz = TimeZone.getTimeZone("UTC")
        def date = new Date().format("yyyyMMdd'T'HHmmss'Z'", tz)

        params.log_output_dir = "${params.output_dir}/log-${manifest.name}-${date}"
        params.output_dir = "${params.output_dir}/${manifest.name}-${manifest.version}"
    }

    set_pipeline_logs = {
        trace.enabled = true
        trace.file = "${params.log_output_dir}/nextflow-log/trace.txt"

        timeline.enabled = true
        timeline.file = "${params.log_output_dir}/nextflow-log/timeline.html"
        
        report.enabled = true
        report.file = "${params.log_output_dir}/nextflow-log/report.html"
    }
    
    set_process = {
        process['withName:call_metapipeline_DNA'].executor = params.executor
        process['withName:call_metapipeline_DNA'].maxForks = params.max_parallel_jobs
        process['withName:call_metapipeline_DNA'].cpus = params.per_job_cpus
        process['withName:call_metapipeline_DNA'].memory = "${params.per_job_memory_GB}GB" as nextflow.util.MemoryUnit
        process['withName:call_metapipeline_DNA'].queue = params.partition
        process['withName:call_metapipeline_DNA'].clusterOptions = "${params.clusterOptions}"
    }

    set_submodule_versions = {
        params.version_BAM2FASTQ  = get_submodule_version('pipeline-convert-BAM2FASTQ')
        params.version_align_DNA  = get_submodule_version('pipeline-align-DNA')
        params.version_call_gSNP  = get_submodule_version('pipeline-call-gSNP')
        params.version_call_sSNV  = get_submodule_version('pipeline-call-sSNV')
        params.version_call_mtSNV = get_submodule_version('pipeline-call-mtSNV')
    }

    set_env = {
        // Ensure leading work dir is set to a directory shared between nodes, eg. /hot
        if (params.ucla_cds) {
            if (! params.leading_work_dir.startsWith('/hot')) {
                throw new Exception("The leading_work_dir must be set to a common directory across nodes (ie. in /hot for ucla_cds). Received ${params.leading_work_dir} instead.")
            }
        }

        schema.check_path(params.leading_work_dir, 'w')
        workDir = params.leading_work_dir

        // Only skip the check if ucla_cds is set to false. In all other cases, perform the check.
        if (params.ucla_cds) {
            if (! params.pipeline_work_dir.startsWith('/scratch')) {
                System.out.println("Pipeline_work_dir is being set to a directory other than /scratch! Please be very careful about I/O operations and network latency!")
            }
        }

        // Setting the work_dir parameter for individual pipelines
        if (params.ucla_cds) {
            /**
             * By default, set the work_dir parameter to /scratch when not specified.
             *
             * WARNING: changing this directory can lead to high server latency and
             * potential disk space limitations. Change with caution! The 'workDir'
             * in Nextflow determines the location of intermediate and temporary files.
             */
            params.work_dir = (params.containsKey("work_dir") && params.work_dir) ? params.work_dir : "/scratch"
            schema.check_path(params.work_dir, 'w')
        } else {
            // If work_dir was specified as a param and exists or can be created, set workDir. Otherwise, let Nextflow's default behavior dictate workDir
            if (params.containsKey("work_dir") && params.work_dir) {
                schema.check_path(params.work_dir, 'w')
            } else {
                // If not ucla_cds and no work_dir is set, leave it as an empty string and let individual pipelines handle workDir
                params.work_dir = ''
            }
        }
    }

// <TODO> add FASTQ support, check that only one of BAM or FASTQ
// check length(params.input), must be 1

    parse_input = {
        if (params?.input) { // if params.input exists then YAML input is used
            if (params.input.size() > 1) {
                throw new Exception("More than one input is specified in the YAML, please choose one of BAM or FASTQ")
            } else {
                return
            }
        } else if (params.containsKey('input_csv')) { // check if CSV input is used
            header_line = reader.readLine().split(',') // reads first line of input csv file and split by comma into list
            print(header_line)
            if (header_line.contains('read1_fastq')) { // for FASTQ csv
                def fastq_input_fields = ['patient', 'sample', 'state', 'site', 'index', 'read_group_identifier', 'sequencing_center', 'library_identifier', 'platform_technology', 'platform_unit', 'bam_header_sm', 'lane', 'read1_fastq', 'read2_fastq']
                params.input.FASTQ = csv_parser.parse_csv(params.input_csv, fastq_input_fields)
            } else if (header_line.contains('path')) { // for BAM csv
                def bam_input_fields = ['patient', 'sample', 'state', 'site', 'path']
                params.input.BAM = csv_parser.parse_csv(params.input_csv, bam_input_fields)
            } else {
                throw new Exception("input CSV does not follow the format for either BAM or FASTQ inputs")
            }
        } else {
            throw new Exception("Neither YAML nor CSV inputs found! Please run pipeline with inputs.")
        }

        /*
        if (params?.input?.BAM) { // this is TRUE if YAML input is used
            return
        } else if (params.containsKey('input_csv')) {
            def bam_input_fields = ['patient', 'sample', 'state', 'site', 'path']
            params.input.BAM = csv_parser.parse_csv(params.input_csv, bam_input_fields)
        } else {
            throw new Exception("Neither YAML nor CSV inputs found! Please run pipeline with inputs.")
        }
        */
    }

    set_up = {
        methods.set_output_dirs()
        methods.set_pipeline_logs()
        methods.set_process()
        methods.set_submodule_versions()
        methods.set_env()
        methods.parse_input()
    }
}
